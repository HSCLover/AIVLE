{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1_UhgsyBLPc"
   },
   "source": [
    "# LangChain1 - Model I-O / Memory / Output Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-wgZVbqcTvD"
   },
   "source": [
    "# 코랩 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SUdky1vUceOx",
    "outputId": "08928299-5ffa-4f82-99c5-227639a1cbc6"
   },
   "outputs": [],
   "source": [
    "# 구글 드라이브 마운트 - 파일 입출력 등에 필요함\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_KRnXRZFcbaq"
   },
   "outputs": [],
   "source": [
    "# 작업 폴더로 이동\n",
    "\n",
    "import os\n",
    "\n",
    "# os.chdir('/content/drive/MyDrive/')\n",
    "\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R6C7UG1lc7SM",
    "outputId": "9e0eb075-4f5a-4bc1-fab8-4e681fa767ce"
   },
   "outputs": [],
   "source": [
    "# 아래 두 명령어는 ipynb 코드 셀에서 파일을 만들고 그 안에 내용을 바로 적을 수 있는 %%writefile 명령어를 수행하기 위해 필요한 설정\n",
    "# 파일 업데이트 시 자동 재업을 위한 설정\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TthEbvtpdQXT",
    "outputId": "9d1ffb92-870e-4269-d0ae-b0577fcbd7f3"
   },
   "outputs": [],
   "source": [
    "%%writefile api_keys.py\n",
    "# api_keys.py 파일을 만들고, 그 안에 OPENAI_API_KEY 변수에 자신의 openai api key를 입력함\n",
    "\n",
    "OPENAI_API_KEY = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9an5oTT_d1qs"
   },
   "outputs": [],
   "source": [
    "# api_keys 모듈에서 OPENAI_API_KEY를 임포트하여 환경 변수에 등록함.\n",
    "# 이렇게 함으로서 OpenAI Model을 사용할 때마다 api key를 전달하는 수고를 덜 수 있음.\n",
    "\n",
    "import os\n",
    "from api_keys import OPENAI_API_KEY\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rbgch2u5BLPc"
   },
   "source": [
    "# 최신 라이브러리 설치\n",
    "- langchain\n",
    "- langchain-core\n",
    "- langchain-community\n",
    "    - langchain thrid-party 라이브러리\n",
    "- langchain-openai\n",
    "    - langchain에서 openai, claude 크고 중요한 모델은 따로 라이브러리를 제공 회사와 함께 관리함.\n",
    "    - 따라서 따로 설치 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gESL1mR3MsMf"
   },
   "source": [
    "# LangChain1 - LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIp9d-npOhTM"
   },
   "source": [
    "## Model I/O - LLM 호출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiWtpHQjifQb"
   },
   "source": [
    "LangChain에서 LLM 호출은 대형 언어 모델(LLM, Large Language Model)을 쉽게 활용할 수 있도록 하는 기능입니다.\n",
    "\n",
    "이를 통해 OpenAI, Hugging Face, Cohere, Claude 등 다양한 LLM을 호출하고, 프롬프트를 생성하며, 체인을 구성하여 복잡한 작업을 수행할 수 있습니다.\n",
    "\n",
    "LangChain은 LLM을 호출하는 데 필요한 인터페이스를 제공하며, 이를 통해 개발자는 모델과의 상호작용을 간단하게 구현할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "eNX1WJoqBWq5",
    "outputId": "3f9d8b1f-0526-4794-e90a-bee09f8ec7dd"
   },
   "outputs": [],
   "source": [
    "# # 실습을 위해 사용할 라이브러리 설치\n",
    "\n",
    "# !pip install -q langchain #← langchain 모듈 설치\n",
    "# !pip install -q langchain-core #← langchain-core 모듈 설치\n",
    "# !pip install -q langchain-community #← langchain-community 모듈 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "acr_kFesBLPd",
    "outputId": "ae765abb-a502-47f7-f874-6b6eb319b9ac"
   },
   "outputs": [],
   "source": [
    "# (예제코드1) Model I/O – LLM 호출\n",
    "# 가장 기본적인 langchain 기능으로 LLM을 호출함.\n",
    "# langchain_community의 chat_models에는 다양한 LLM이 모여 있음.\n",
    "\n",
    "from langchain_community.chat_models import ChatOpenAI  #ChatOpenAI 클래스 가져오기\n",
    "\n",
    "chat = ChatOpenAI(  #← 클라이언트를 만들고 chat에 저장\n",
    "    model=\"gpt-4o-mini\",  #← 호출할 모델 지정\n",
    ")\n",
    "\n",
    "res = chat.invoke('너는 누구니?')\n",
    "print(res.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "BXf7Qq5XAjVX",
    "outputId": "b6f7b8e8-0506-4fa4-8f34-7c597fc78d97"
   },
   "outputs": [],
   "source": [
    "# AIMessage 클래스의 pretty_print 메서드: text를 보기 쉽게 출력해 줌.\n",
    "\n",
    "res.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "5mS52migH1xt",
    "outputId": "3ae701bc-e955-41b8-e8df-dcc6cc4bfded"
   },
   "outputs": [],
   "source": [
    "# langchain-openai 설치\n",
    "# langchain_community.chat_models를 사용해도 되지만, 일부 인기 있는 LLM은 따로 Integration으로 라이브러리를 모아놓음.\n",
    "# 만약 Anthropic의 LLM을 사용하고 싶으면, 다음 명령어를 수행할 것.\n",
    "# pip install -U langchain-anthropic\n",
    "# export ANTHROPIC_API_KEY=\"your-api-key\"\n",
    "\n",
    "# !pip install langchain #← langchain 모듈 설치\n",
    "# !pip install langchain-core #← langchain-core 모듈 설치\n",
    "# !pip install langchain-openai #← Integration 패키지에서 langchain-openai 모듈 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LEBEDSNJHx4k",
    "outputId": "7be25a91-9878-40d6-bdb3-b2d431424bf2"
   },
   "outputs": [],
   "source": [
    "# (예제코드2) Model I/O – LLM 호출\n",
    "# 위 예시와 동일한 예시지만, 사용하는 Class가 다름.\n",
    "# 이건 OpenAI Integration 모듈을 사용함. 그러나 결과는 같음\n",
    "\n",
    "from langchain_openai import ChatOpenAI  #← ChatOpenAI 클래스 가져오기\n",
    "\n",
    "chat = ChatOpenAI(  #← 클라이언트를 만들고 chat에 저장\n",
    "    model=\"gpt-3.5-turbo\",  #← 호출할 모델 지정\n",
    "    temperature=0.1,\n",
    "    # max_tokens=512\n",
    ")\n",
    "\n",
    "res = chat.invoke('생성형 AI에 대해 50자 이내로 간략히 설명해줘')\n",
    "print(res.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "oT-bTO8cFp-p",
    "outputId": "8715d175-cb6f-447b-cd64-e3a04e872df3"
   },
   "outputs": [],
   "source": [
    "# 스트리밍으로 출력받기\n",
    "\n",
    "for string in chat.stream('생성형 AI에 대해 50자 이내로 간략히 설명해줘'):\n",
    "    string.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKf3NGFOxUIw"
   },
   "source": [
    "### 연습문제1) ChatOpenAI 클래스를 이용하여 다음 조건을 만족하는 Chat Model을 만들고, 다음 질문에 대한 추론 결과를 프린트 하세요.\n",
    "- 모델: gpt-4o\n",
    "- 창의성: 1.5\n",
    "- 최대토큰 수: 100\n",
    "- 질문: 화성의 기후와 환경을 고려하여 만약 화성에 외\n",
    "계인이 존재한다면, 어떤 모습일지 상상해봐. 한국어로\n",
    "답변해줘."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dhUNhwE_xUIw",
    "outputId": "0fdfb62e-d383-45f2-8a09-3503e76bf338"
   },
   "outputs": [],
   "source": [
    "# (연습문제1) Model I/O – LLM 호출\n",
    "# 이번 연습문제는 Chat_Model 객체를 만들고, 질의를 하고, 답변을 받아내는 일련의 과정을 진행하는 것.\n",
    "# 입력 인자의 타입이 반드시 Prompt일 필요는 없음. 문자열로 입력해도 됨.\n",
    "\n",
    "from langchain_openai import ChatOpenAI  # ChatOpenAI 클래스 가져오기\n",
    "\n",
    "# Chat Model 생성\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-4o\",  # 사용할 모델 지정\n",
    "    temperature=1.5,  # 창의성 증가\n",
    "    max_tokens=100  # 최대 토큰 수 제한\n",
    ")\n",
    "\n",
    "# 질문 정의\n",
    "question = \"화성의 기후와 환경을 고려하여 만약 화성에 외계인이 존재한다면, 어떤 모습일지 상상해봐. 한국어로 답변해줘\"\n",
    "\n",
    "# 모델 호출 및 결과 출력\n",
    "response = chat.invoke(question)\n",
    "print(type(response))\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6UVpeiIijmS"
   },
   "source": [
    "## Model I/O - Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "nj34J1TalJaa",
    "outputId": "0d1d1784-fcbb-48a7-89f5-8e4e8d81d987"
   },
   "outputs": [],
   "source": [
    "# (예제코드3) Model I/O – HumanMessage 사용\n",
    "# AIMessage와 HumanMessage를 사용한 경우 - 대화를 이어가고 이전 대화를 기억하는 것을 구현\n",
    "# Chat_Model은 한번의 질의문만 할 수 있는 게 아니고, 대화를 이어나갈 수 있음.\n",
    "# 이때 사용하는 가장 간단한 방법은, HumanMessage, AIMessage 객체를 이용하여 대화를 만들고, 이를 리스트에 묶어서 Chat_model에 전달함.\n",
    "# 물론 여기 예제는 정해진 질문과 답변이 나오게 설계되어 있지만, 전반적인 프로세스는 Message 객체를 활용하면 됨.\n",
    "\n",
    "# !pip install langchain #← langchain 모듈 설치\n",
    "# !pip install langchain-core #← langchain-core 모듈 설치\n",
    "# !pip install langchain-openai #← Integration 패키지에서 langchain-openai 모듈 설치\n",
    "\n",
    "from langchain_openai import ChatOpenAI #← ChatOpenAI 클래스 가져오기\n",
    "from langchain_core.messages import HumanMessage #← 사용자의 메시지인 HumanMessage 가져오기\n",
    "\n",
    "chat = ChatOpenAI( #← 클라이언트를 만들고 chat에 저장\n",
    "    model=\"gpt-4o-mini\", #← 호출할 모델 지정\n",
    ")\n",
    "res = chat( #← 실행하기\n",
    "    [\n",
    "        HumanMessage(content=\"안녕하세요!\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(type(res))\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "DpMvyZJomSxw",
    "outputId": "23668612-c187-439d-e619-56c6a45ac866"
   },
   "outputs": [],
   "source": [
    "# (예제코드4) Model I/O – HumanMessage, SystemMessage사용\n",
    "# 이번에는 SystemMessage 사용법을 보여주는 예시인데, 이는 마치 prompt에서 AI의 페르소나를 설정해 주는 것과 비슷한 역할을 함.\n",
    "# 또는 AI의 역할과 해야할 것, 하지 말아야 할 것 등을 제시하여, 원하는 결과를 얻도록 장치하는 것임.\n",
    "\n",
    "# !pip install langchain #← langchain 모듈 설치\n",
    "# !pip install langchain-core #← langchain-core 모듈 설치\n",
    "# !pip install langchain-openai #← Integration 패키지에서 langchain-openai 모듈 설치\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"당신의 인공지능 챗봇이 아니야. 당신은 나의 친구 AIglue야.\"),\n",
    "    HumanMessage(\n",
    "        content=\"너는 누구니?\")\n",
    "]\n",
    "\n",
    "result = chat.invoke(messages)\n",
    "result.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "xDM5-idEi7VI",
    "outputId": "00a2787f-99a5-46df-e4c3-1809027c47b2"
   },
   "outputs": [],
   "source": [
    "# (예제코드5) Model I/O – HumanMessage, AIMessage사용\n",
    "# langchain 프롬프트에서 중괄호는 \"변수\" 기능이 있음.\n",
    "# 즉, 중괄호 안의 텍스트를 곧이곧대로 해석하지 말고, 어떤 값의 변수 역할을 하는데, 변수의 기능을 간접적으로 설명하는 텍스트가 사용됨.\n",
    "# 아래 예시는 AI가 \"계란찜 만드는 법\"을 어떻게 설명할지 모르지만, 그 내용을 AIMessage로 넣겠다느 의지가 반영되어 있음.\n",
    "\n",
    "# !pip install langchain #← langchain 모듈 설치\n",
    "# !pip install langchain-core #← langchain-core 모듈 설치\n",
    "# !pip install langchain-openai #← Integration 패키지에서 langchain-openai 모듈 설치\n",
    "\n",
    "from langchain_openai import ChatOpenAI  #← 모듈 가져오기\n",
    "from langchain_core.messages import HumanMessage, AIMessage  #← 사용자의 메시지인 HumanMessage 가져오기\n",
    "\n",
    "chat = ChatOpenAI(  #← 클라이언트를 만들고 chat에 저장\n",
    "    model=\"gpt-4o-mini\",  #← 호출할 모델 지정\n",
    ")\n",
    "\n",
    "result = chat.invoke( #← 실행하기\n",
    "    [\n",
    "        HumanMessage(content=\"계란찜 만드는 법 알려줘\"),\n",
    "        AIMessage(content='{ChatModel의 답변인 계란찜 만드는 법}'), # 답변을 단기적으로 상기시켜주는 역할을 함. assistant 역할.\n",
    "        HumanMessage(content='일본어로 번역해줘')\n",
    "    ]\n",
    ")\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r577xyaYBLPe",
    "outputId": "1f2e1acd-6206-4bb4-b398-8d891b352e54"
   },
   "outputs": [],
   "source": [
    "# SystemMessage 추가\n",
    "# SystemMessage를 통해, LLM의 페르소나를 설정함.\n",
    "\n",
    "from langchain_openai import ChatOpenAI  #← 모듈 가져오기\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "chat = ChatOpenAI(  #← 클라이언트를 만들고 chat에 저장\n",
    "    model=\"gpt-4o-mini\",  #← 호출할 모델 지정\n",
    ")\n",
    "\n",
    "result = chat.invoke( #← 실행하기\n",
    "    [\n",
    "        SystemMessage(content='당신은 고급 레스토랑의 쉐프입니다. 아주 간략하게 요리의 핵심을 파악해서 100자 이내로 조리법을 설명해 줍니다.'),\n",
    "        HumanMessage(content=\"계란찜 만드는 법 알려줘\"),\n",
    "        AIMessage(content='{ChatModel의 답변인 계란찜 만드는 법}'), # 답변을 단기적으로 상기시켜주는 역할을 함. assistant 역할.\n",
    "        HumanMessage(content='영어로 번역해줘')\n",
    "    ]\n",
    ")\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKq2MNK6xUIy"
   },
   "source": [
    "### 연습문제2) ChatOpenAI 클래스와 Messages를 이용하여 다음 조건을 만족하는 Chat Model을 만들고, 다음 질문에 대한 추론 결과를 프린트 하세요.\n",
    "- 모델: gpt-3.5-turbo-16k\n",
    "- 창의성: 0.2\n",
    "- 최대토큰 수: 300\n",
    "- AI 페르소나: 고급 레스토랑 쉐프. 추가적인 설정 환영\n",
    "- 지시사항1: 만우절에 어울리는 새로운 이탈리안 음식 개발하기\n",
    "- 지시사항2: AI의답변에 이어, 개발한 메뉴를 레시피로 정리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "SC9SwkYPxUIy"
   },
   "outputs": [],
   "source": [
    "# (연습문제2) Model I/O – Messages\n",
    "# 앞에서 배운 Message 객체의 여러 종류들의 의미를 알고, 추론에 사용해 본다.\n",
    "# 중괄호의 의미를 이해하고 적절히 활용한다.\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Chat Model 생성\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo-16k\",  # 모델 지정\n",
    "    temperature=0.2,  # 창의성 설정 (논리적인 답변을 유지)\n",
    "    max_tokens=300  # 최대 토큰 수 제한\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"당신은 미슐랭 3스타를 보유한 고급 이탈리안 레스토랑의 쉐프입니다. 창의적이면서도 현실적인 요리를 개발하는 것이 중요합니다.\"), # AI 페르소나 설정\n",
    "    HumanMessage(content=\"만우절에 어울리는 새로운 이탈리안 음식을 개발해줘.\"), # AI에게 만우절에 어울리는 새로운 이탈리안 음식 개발 요청\n",
    "    AIMessage(content='{ChatModel의 답변인 만우절에 어울리는 새로운 메뉴}'), # AI의 답변 기억\n",
    "    HumanMessage(content=\"개발한 이 메뉴의 레시피를 정리해줘.\") # 신메뉴 레시피 정리 요청\n",
    "]\n",
    "\n",
    "# 모델 실행\n",
    "response = chat(messages)\n",
    "\n",
    "# 결과 출력\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6CKnRJBxUIy"
   },
   "source": [
    "## Model I/O - PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "8edeb9UyxUIy",
    "outputId": "bafb9855-422f-4ac7-ef1a-9e8afbf97d47"
   },
   "outputs": [],
   "source": [
    "# (예제코드6) Model I/O – PromptTemplate 기본\n",
    "# PromptTemplate의 기본 사용법을 예시로 보여줌.\n",
    "\n",
    "# !pip install langchain #← langchain 모듈 설치\n",
    "# !pip install langchain-core #← langchain-core 모듈 설치\n",
    "# !pip install langchain-openai #← Integration 패키지에서 langchain-openai 모듈 설치\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template=\"안녕하세요, 제 이름은 {name}입니다. 저는 {hobby}을 좋아합니다.\",\n",
    "    input_variables=[\"name\", \"hobby\"],\n",
    ")\n",
    "\n",
    "prompt_text = template.format(name=\"AIglue\", hobby=\"배드민턴\")\n",
    "print(type(template))\n",
    "print(type(prompt_text))\n",
    "print(prompt_text)\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo-16k\",  # 모델 지정\n",
    "    temperature=0.2,  # 창의성 설정 (논리적인 답변을 유지)\n",
    "    max_tokens=300  # 최대 토큰 수 제한\n",
    ")\n",
    "\n",
    "res = chat.invoke(prompt_text)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "4TbfH6IAxUIy",
    "outputId": "8792a24d-296a-448a-c0f4-11de696424a6"
   },
   "outputs": [],
   "source": [
    "# (예제코드7) Model I/O – PromptTemplate + LLM 호출\n",
    "# LLM 호출에 문자열이나 Message 객체를 직접적으로 사용하지 않고, PromptTemplate을 만들어서 사용함.\n",
    "# PromptTemplate의 장점은 고정된 질의문이 아니라 변수를 이용하여 질의문을 변형할 수 있다는 것.\n",
    "\n",
    "# !pip install langchain #← langchain 모듈 설치\n",
    "# !pip install langchain-core #← langchain-core 모듈 설치\n",
    "# !pip install langchain-openai #← Integration 패키지에서 langchain-openai 모듈 설치\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# OpenAI 모델 생성\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "template = PromptTemplate.from_template(\"역사 속 {event}에 대해 100자 이내로 설명해줘.\")\n",
    "\n",
    "# 프롬프트 적용 후 LLM 실행\n",
    "prompt_text = template.format(event=\"프랑스 혁명\")\n",
    "res = chat.invoke(prompt_text)\n",
    "\n",
    "print(res.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "g4rEr4TSBLPe",
    "outputId": "7e8c0cb3-1251-4aed-8aba-925b16098576"
   },
   "outputs": [],
   "source": [
    "# (예제코드8) Model I/O – PromptTemplate + Message + LLM 호출\n",
    "# PromptTemplate으로 질의문을 만들고, 이를 Message 객체 넣어서 LLM을 호출하는 방식\n",
    "# 반드시 이렇게 해야하는 건 아니지만, 각 객체의 역할이 따로 있음.\n",
    "# PromptTemplate으로 변수 기반의 질문을 만듦.\n",
    "# 이를 AIMessage, HummanMessage, SystemMessage에 넣어 연결되는 대화문에 활용\n",
    "\n",
    "# !pip install langchain #← langchain 모듈 설치\n",
    "# !pip install langchain-core #← langchain-core 모듈 설치\n",
    "# !pip install langchain-openai #← Integration 패키지에서 langchain-openai 모듈 설치\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "chat = ChatOpenAI(  #← 클라이언트 생성 및 chat에 저장\n",
    "    model=\"gpt-3.5-turbo\",  #← 호출할 모델 지정\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(  #← PromptTemplate을 작성\n",
    "    template=\"{company}의 주력 제품은 무엇인가요？\",  #← {company}라는 변수를 포함하는 프롬프트 작성하기\n",
    "    input_variables=[\n",
    "        \"company\"  #← company에 입력할 변수 지정\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = chat.invoke( #← 실행\n",
    "    [\n",
    "        SystemMessage(content='당신은 유능한 기업 분석가입니다.'),\n",
    "        HumanMessage(content=prompt.format(company=\"sk hynix\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_GNduuPxUIy"
   },
   "source": [
    "### 연습문제3) 당신은 한 기업의 영업 직원이며, 신제품을 고객들에게 효과적으로 설명하는 역할을 맡고 있습니다. 회사의 제품은 다양하기 때문에, 제품 이름만 입력하면 AI가 자동으로 해당 제품의 특징을 설명해주는 도우미 시스템을 구축하려 합니다. 다음 요구사항을 고려하여 Prompt를 설계하고, LLM을 호출하는 코드를 작성하세요.\n",
    "- 제품명만 입력하면, AI가 자동으로 해당 제품을 상세히 설명하는 시스템 구축\n",
    "- AI에게 전문적인 영업 컨설턴트 역할 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "nqIxjrcexUIy",
    "outputId": "fb197d3d-bd9b-429b-bd0e-8f4a79abacc8"
   },
   "outputs": [],
   "source": [
    "# (연습문제3) Model I/O – PromptTemplate\n",
    "# 앞에서 배운 PromptTemplate, Message, LLM 호출 기능을 활용하여 LLM으로부터 답변을 얻어냄.\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",  # 모델 지정\n",
    "    temperature=0.7  # 창의성을 약간 증가하여 더 매력적인 제품 설명 유도\n",
    ")\n",
    "\n",
    "# product_prompt = PromptTemplate(\n",
    "#     template=\"고객에게 {product}를 설명하세요. 이 제품의 주요 특징과 고객이 이를 사용해야 하는 이유를 강조하세요.\",\n",
    "#     input_variables=[\"product\"]\n",
    "# )\n",
    "\n",
    "product_prompt = PromptTemplate.from_template(\n",
    "    template=\"고객에게 {product}를 설명하세요. 이 제품의 주요 특징과 고객이 이를 사용해야 하는 이유를 강조하세요.\",\n",
    "    )\n",
    "\n",
    "result = chat(\n",
    "    [\n",
    "        SystemMessage(content=\"당신은 뛰어난 영업 전문가이며, 제품을 고객에게 매력적으로 설명하는 능력을 갖추고 있습니다.\"),\n",
    "        HumanMessage(content=product_prompt.format(product=\"스마트 공기 청정기\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0in2vbJ9xUIy"
   },
   "source": [
    "## Model I/O - Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "yp0vMShHxUIy",
    "outputId": "6d4d5a14-f033-4043-8369-c56a1ceabd0c"
   },
   "outputs": [],
   "source": [
    "# (예제코드9) Model I/O – Output Parser(StrOutputParser)\n",
    "# Output Parser 중 가장 간단한 파싱 기법\n",
    "\n",
    "# !pip install langchain #← langchain 모듈 설치\n",
    "# !pip install langchain-core #← langchain-core 모듈 설치\n",
    "# !pip install langchain-openai #← Integration 패키지에서 langchain-openai 모듈 설치\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "res = chat.invoke(\"AI의 역할은 무엇인가요?\")\n",
    "parsed_output = parser.parse(res.content)\n",
    "\n",
    "print(type(parsed_output))\n",
    "print(parsed_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "0R7Yhdx9xUIz",
    "outputId": "d30f9102-7a22-42dd-bd5f-f8592136df80"
   },
   "outputs": [],
   "source": [
    "# (예제코드10) Model I/O – Output Parser(JsonOutputParser)\n",
    "# JSON 형식으로 제공해 달라는 요청이 있어야 만이 output Parser를 작동시킬 수 있음.\n",
    "\n",
    "# !pip install langchain #← langchain 모듈 설치\n",
    "# !pip install langchain-core #← langchain-core 모듈 설치\n",
    "# !pip install langchain-openai #← Integration 패키지에서 langchain-openai 모듈 설치\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=(\n",
    "        \"대표적인 프로그래밍 언어 3가지를 JSON 형식으로 제공해줘. \"\n",
    "        \"반드시 순수 JSON만 반환해야 하며, 다른 설명은 포함하지 마. \" # 이 문구가 중요. AI 출력이 JSON 형식이 아니면 에러 발생\n",
    "        \"형식: {{\\\"languages\\\": [\\\"언어1\\\", \\\"언어2\\\", \\\"언어3\\\"]}}\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat.invoke(prompt.format())\n",
    "parsed_output = parser.parse(res.content)\n",
    "\n",
    "print(type(parsed_output))\n",
    "print(parsed_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "aS-KYioVxUIz",
    "outputId": "eb48cd86-db5e-4a4d-fc80-0e9ad2bf755c"
   },
   "outputs": [],
   "source": [
    "# (예제코드11) Model I/O – Output Parser(ListOutputParser)\n",
    "# LLM의 답변을 List 형식으로 받아내고 이를 처리하고 표현함.\n",
    "\n",
    "# !pip install langchain #← langchain 모듈 설치\n",
    "# !pip install langchain-core #← langchain-core 모듈 설치\n",
    "# !pip install langchain-openai #← Integration 패키지에서 langchain-openai 모듈 설치\n",
    "\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# OpenAI 모델 생성\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# ListOutputParser 설정\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# 프롬프트 템플릿 설정\n",
    "prompt = PromptTemplate(\n",
    "    template=\"가장 인기 있는 프로그래밍 언어 5가지를 리스트 형식으로 제공해줘. \\n- {format_instructions}\",\n",
    "    input_variables=[],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# 모델 실행\n",
    "response = chat.invoke(prompt.format())\n",
    "\n",
    "# 리스트로 변환 (파싱)\n",
    "parsed_output = parser.parse(response.content)\n",
    "\n",
    "# 결과 타입 출력\n",
    "print(type(parsed_output))\n",
    "\n",
    "# 결과 출력\n",
    "print(parsed_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "6lLAFpT-xUIz",
    "outputId": "c695768b-bcf3-4b72-f197-cd0c6c965a0b"
   },
   "outputs": [],
   "source": [
    "# LangChain의 Pydantic Parser를 사용하려면 pydantic이 설치되어 있어야 함.\n",
    "\n",
    "# !pip install \"pydantic[email]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Ry0pzugaxUIz",
    "outputId": "6b6df41c-e385-4217-cb6e-aaf606a95fad"
   },
   "outputs": [],
   "source": [
    "# (예제코드12) Model I/O – Output Parser(PydanticOutputParser)\n",
    "# Pydantic 모델을 만들어서 Output 형태를 사용자가 마음대로 설정할 수 있는 것이 장점\n",
    "# 아래 prompt에서 partial_varialbes는 pydantic model 형식을 미리 지정해 둔 것으로, 이를 기반하여 LLM이 답변을 생성할 수 있도록 유도함.\n",
    "\n",
    "# !pip install langchain #← langchain 모듈 설치\n",
    "# !pip install langchain-core #← langchain-core 모듈 설치\n",
    "# !pip install langchain-openai #← Integration 패키지에서 langchain-openai 모듈 설치\n",
    "# !pip install \"pydantic[email]\" #← pydantic email-validation을 위한 모듈 설치\n",
    "\n",
    "from typing import List\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class MovieInfo(BaseModel):\n",
    "    title: str = Field(..., description=\"영화 제목\")\n",
    "    director: str = Field(..., description=\"영화 감독\")\n",
    "    genre: str = Field(..., description=\"영화 장르\")\n",
    "    actors: List[str] = Field(..., description=\"출연 배우 이름 목록\")\n",
    "    release_year: int = Field(..., description=\"개봉 연도\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=MovieInfo)\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"영화 '{movie_name}'에 대한 정보를 JSON 형식으로 제공해줘. {format_instructions}\",\n",
    "    input_variables=[\"movie_name\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "res = chat.invoke(prompt.format(movie_name=\"인셉션\"))\n",
    "\n",
    "parsed_output = parser.parse(res.content)\n",
    "\n",
    "print(type(parsed_output))\n",
    "print('-'*100)\n",
    "print(parsed_output)\n",
    "print('-'*100)\n",
    "print(parsed_output.model_dump())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnZHvabnxUIz"
   },
   "source": [
    "### 연습문제4) 아래 조건을 만족하는 Pydantic 모델 TouristSpotInfo을 정의하고, LLM의 출력을 파싱하세요.\n",
    "- TouristSpotInfo 모델을 Pydantic을 이용하여 정의하세요.\n",
    "    - name: 관광지의 이름 (문자열)\n",
    "    - country: 관광지가 위치한 국가 (문자열)\n",
    "    - city: 관광지가 위치한 도시 (문자열)\n",
    "    - attractions: 관광지의 주요 볼거리 목록 (문자열 리스트)\n",
    "    - best_visit_season: 가장 적절한 방문 계절 (문자열)\n",
    "- LangChain의 PydanticOutputParser를 사용하여 JSON 데이터를 파싱하세요.\n",
    "- 사용자가 특정 관광지를 입력하면, 해당 관광지 정보를 LLM을 활용하여 생성하고, 이를 TouristSpotInfo 모델로 변환하세요.\n",
    "- 변환된 데이터를 출력하고, 모델의 JSON 변환 결과를 출력하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bGvmMfIUxUIz",
    "outputId": "4bd5f198-2d3f-4c93-88ae-f05933ed90e9"
   },
   "outputs": [],
   "source": [
    "# (연습문제4) Model I/O – Output Parser – PydanticOutput Parser\n",
    "# Pydantic Model을 정의하고, template에서 pydantic model 답변을 요구함.\n",
    "\n",
    "from typing import List\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class TouristSpotInfo(BaseModel):\n",
    "     name: str = Field(..., description=\"관광지 이름\")\n",
    "     country: str = Field(..., description=\"관광지가 위치한 국가\")\n",
    "     city: str = Field(..., description=\"관광지가 위치한 도시\")\n",
    "     attractions: List[str] = Field(..., description=\"주요 볼거리 목록\")\n",
    "     best_visit_season: str = Field(..., description=\"가장 적절한 방문 계절\")\n",
    "     year:int = Field(..., description=\"완성된 시기\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=TouristSpotInfo)\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "prompt = PromptTemplate(\n",
    "    template=\"관광지 '{tourist_spot}'에 대한 정보를 JSON 형식으로 제공해줘. {format_instructions}\",\n",
    "    input_variables=[\"tourist_spot\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "res = chat.invoke(prompt.format(tourist_spot=\"에펠탑\"))\n",
    "parsed_output = parser.parse(res.content)\n",
    "print(parsed_output)\n",
    "print(parsed_output.model_dump())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "imb3Uh99qUGl",
    "outputId": "550e9092-dea0-4f4e-e5b3-f94618619a05"
   },
   "outputs": [],
   "source": [
    "# (비교-연습문제4) Model I/O – Output Parser를 사용하지 않는 경우\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"관광지 '{tourist_spot}'에 대한 정보를 제공해줘\",\n",
    "    input_variables=[\"tourist_spot\"],\n",
    ")\n",
    "\n",
    "res = chat.invoke(prompt.format(tourist_spot=\"에펠탑\"))\n",
    "print(res.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vaOn_lSxUIz"
   },
   "source": [
    "### 연습문제5) 아래 조건을 만족하는 Pydantic 모델 User를 정의하고, LLM의 출력을 파싱하세요.\n",
    "- “name”: 문자열이고, 최소 2글자 이상 입력되어야 함.\n",
    "- “age”: 정수이며, 18이상 100 이하의 값이어야 함.\n",
    "- “email”: 올바른 이메일 형식이어야 함.\n",
    "- 적절한 예외 처리를 작성하여 오류 메시지 출력\n",
    "- 사용자 입력 정보(name, age, email)을 전달하는 프롬프트를 만들고, LLM 출력 결과를 User 모델로 파싱하기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nRTyu7x0xUIz",
    "outputId": "9bfcd8c4-f4a0-4de9-9d7e-3ca119b60530"
   },
   "outputs": [],
   "source": [
    "# (연습문제5-심화) Model I/O – Output Parser\n",
    "# pydantic model을 만들 때, ge, le 등 생소한 파라미터가 있는 것에 유의할 것.\n",
    "\n",
    "from pydantic import BaseModel, EmailStr, Field, ValidationError\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "class User(BaseModel):\n",
    "    name: str = Field(..., min_length=2, description=\"이름은 최소 2글자 이상이어야 합니다.\")\n",
    "    age: int = Field(..., ge=18, le=100, description=\"나이는 18 이상 100 이하의 정수여야 합니다.\")\n",
    "    email: EmailStr = Field(..., description=\"올바른 이메일 형식이어야 합니다.\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=User)\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"사용자의 입력 정보 - 이름, 이메일주소, 나이: \\n{user_input}\\n을 JSON 형식으로 변환하세요. \\n{format_instructions}\",\n",
    "    input_variables=[\"user_input\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "user_input = input('이름, 이메일주소, 나이를 입력해 주세요. 멤버 등록을 위한 필수 사항입니다. ^^\\n')\n",
    "\n",
    "res = chat.invoke(prompt.format(user_input=user_input))\n",
    "\n",
    "try:\n",
    "    parsed_data = parser.parse(res.content)\n",
    "    print(\"데이터 검증 성공:\", parsed_data.model_dump())\n",
    "except:\n",
    "    print(f\"데이터 검증 실패:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpyJimfiBLPv"
   },
   "source": [
    "# LangChain2 - Memory\n",
    "- 단기 기억\n",
    "    - langchain.llm_cache = InMemoryCache() : 이렇게 해도 되지만,\n",
    "    - ChatOpenAI의 cache 옵션에서 처리하면 더 깔끔함.\n",
    "    - inMemoryCache를 사용하면 응답이 훨씬 빠름.\n",
    "- 장기기억\n",
    "    - Vector Database 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ECM6EIcbxUI0",
    "outputId": "6e6296ce-440c-432b-b28d-3482afe56251"
   },
   "outputs": [],
   "source": [
    "# (예제코드1) Memory - InMemoryCache\n",
    "# 컴퓨터 메모리를 이용하여 대화 내용을 단기 기억하는 방식\n",
    "\n",
    "# !pip install langchain #← langchain 모듈 설치\n",
    "# !pip install langchain-core #← langchain-core 모듈 설치\n",
    "# !pip install langchain-openai #← Integration 패키지에서 langchain-openai 모듈 설치\n",
    "# !pip install -qU langchain-community #← langchain-community 모듈 설치\n",
    "\n",
    "import time  #← 실행 시간을 측정하기 위해 time 모듈 가져오기\n",
    "import langchain\n",
    "from langchain.cache import InMemoryCache  #← InMemoryCache 가져오기\n",
    "from langchain_openai import ChatOpenAI # 이렇게 import 해도 되고, from langchain.chat_models import ChatOpenAI로 해도 됨.\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "chat = ChatOpenAI(model='gpt-4o-mini',\n",
    "    max_tokens=300,\n",
    "    cache=InMemoryCache())  #← OpenAI 클라이언트 생성\n",
    "\n",
    "start = time.time() #← 실행 시작 시간 기록\n",
    "result = chat.invoke([ #← 첫 번째 실행을 수행\n",
    "    HumanMessage(content=\"안녕하세요!\")\n",
    "])\n",
    "\n",
    "end = time.time() #← 실행 종료 시간 기록\n",
    "print(result.content)\n",
    "print(f\"실행 시간: {end - start}초\")\n",
    "\n",
    "start = time.time() #← 실행 시작 시간 기록\n",
    "result = chat.invoke([ #← 같은 내용으로 두 번째 실행을 함으로써 캐시가 활용되어 즉시 실행 완료됨\n",
    "    HumanMessage(content=\"안녕하세요!\")\n",
    "])\n",
    "\n",
    "end = time.time() #← 실행 종료 시간 기록\n",
    "print(result.content)\n",
    "print(f\"실행 시간: {end - start}초\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "mw_iOaLxBLPw",
    "outputId": "697e5b3f-e317-4f33-fb52-1e4755c61e44"
   },
   "outputs": [],
   "source": [
    "# (예제코드2) Memory - ConversationBufferMemory\n",
    "# 대화 내용을 쉽게 저장하도록 만든 클래스\n",
    "# input은 사용자의 입력, output은 AI의 답변이라고 생각해도 됨.\n",
    "# Message 객체를 저장함.\n",
    "\n",
    "# !pip install langchain #← langchain 모듈 설치\n",
    "# !pip install langchain-core #← langchain-core 모듈 설치\n",
    "# !pip install langchain-openai #← Integration 패키지에서 langchain-openai 모듈 설치\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory( #← 메모리 초기화\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "memory.save_context( #← 메모리에 메시지를 추가\n",
    "    {\n",
    "        \"input\": \"안녕하세요!\"\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"안녕하세요! 잘 지내고 계신가요? 궁금한 점이 있으면 알려 주세요. 어떻게 도와드릴까요?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "memory.save_context( #← 메모리에 메시지를 추가\n",
    "    {\n",
    "        \"input\": \"오늘 날씨가 좋네요\"\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"저는 AI이기 때문에 실제 날씨를 느낄 수는 없지만, 날씨가 좋은 날은 외출이나 활동을 즐기기에 좋은 날입니다!\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(type(memory))\n",
    "\n",
    "history = memory.load_memory_variables({})['history']\n",
    "\n",
    "for story in history:\n",
    "    print(f'{type(story)}: {story.content}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtqvrCVfBLP5"
   },
   "source": [
    "# LangChain3 - Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1G11gX_pBLP5",
    "outputId": "f427d4b3-68ab-4742-8206-08f8289b2df0"
   },
   "outputs": [],
   "source": [
    "# (예제코드1) Chains - LLMChain\n",
    "# 앞서 여러 Chain을 살펴 보았지만, LLMChain은 가장 기본적인 Prompt와 LLM을 연결하는 Chain임.\n",
    "# Chain으로 묶지 않고, LLM으로 Prompt를 바로 입력해도 됨.\n",
    "\n",
    "from langchain import LLMChain, PromptTemplate  #← LLMChain 가져오기\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"{product}는 어느 회사에서 개발한 제품인가요?\",\n",
    "    input_variables=[\n",
    "        \"product\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 예전 버전의 방식(비추)\n",
    "chain = LLMChain( #← LLMChain을 생성\n",
    "    llm=chat,\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "result = chain.invoke(input={'product':\"iPhone\"})\n",
    "\n",
    "print(result)\n",
    "print(type(result))\n",
    "\n",
    "print(result['text'])\n",
    "print('-'*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KUY5UJxVum26",
    "outputId": "293afed8-606a-440a-9ff9-0630b97e049e"
   },
   "outputs": [],
   "source": [
    "# # (예제코드1) Chains - LLMChain - 현재 추천하는 방식\n",
    "\n",
    "chain = prompt | chat\n",
    "result = chain.invoke(input={'product':\"iPhone\"})\n",
    "\n",
    "print(type(result))\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "__RxVU56BLP6",
    "outputId": "ce18eb4d-955a-4e57-be27-6aeb9672dffe"
   },
   "outputs": [],
   "source": [
    "# (예제코드2) Chains - ConversationChain + ConversationBufferMemory\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "chain = ConversationChain( #← ConversationChain을 초기화\n",
    "    memory=memory, #← Memory 모듈을 지정\n",
    "    llm=chat, #← 언어 모델을 지정\n",
    ")\n",
    "\n",
    "res = chain.invoke(\"안녕하세요!\") #← ConversationChain을 실행\n",
    "print(type(res))\n",
    "print(res)\n",
    "# print(res['history'][-1].content)\n",
    "print(res['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMvGrYUSBLP6"
   },
   "source": [
    "## 특정 기능에 특화된 Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0TWAa8zBLP6"
   },
   "source": [
    "### 특정 URL에 접속해 정보 얻는 방법\n",
    "- url 접속 기능(LLMRequestsChain)\n",
    "- LLMChain\n",
    "- process\n",
    "    - 웹 요청: LLMRequestsChain이 url에 요청을 보내 json 데이터를 가져옴.\n",
    "    - LLM 호출: 가져온 데이터를 문자열로 바꿔 requests_result 변수에 매핑하여 PromptTemplate을 채움.\n",
    "    - LLM 응답: 채워진 프롬프트가 LLM에 전달되어 응답을 생성\n",
    "- 그러니까 LLMRequestsChain의 변수 requests_result는 드러나지 않기 때문에 미리 알고 있어야 함.\n",
    "- url 변수도 마찬가지임. query는 사용자가 정의한 변수지만, url은  LLMRequestsChain에서 정의한 변수임.\n",
    "- chains 마다 입력 받는 변수와 다음 chain에 전달하는 변수를 잘 파악해야 함.\n",
    "- 모든 chain이 다 그렇진 않겠지만, 이번 케이스는 LLMChain보다 LLMRequestsChain이 먼저 실행되어야 함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K8yR6_XKBLP6",
    "outputId": "2945bb55-2a79-488d-f6b3-0646bbc59245"
   },
   "outputs": [],
   "source": [
    "# bs4 라이브러리 설치\n",
    "# !pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "5zyYvORzBLP6",
    "outputId": "11dbe797-7eed-4bae-c669-aeedd84ef410"
   },
   "outputs": [],
   "source": [
    "# (예제코드3) Chains - LLMRequestsChain\n",
    "# prompt와 llm을 연결하여 llm_chain을 만들고, 이 chain을 URL에 접근하여 JSON 형태로 데이터를 가져오는 LLMRequestsChain에 입력하여 받아온 데이터를 기반으로 응답하는 시스템\n",
    "# reference: https://python.langchain.com/api_reference/_modules/langchain_community/chains/llm_requests.html#LLMRequestsChain\n",
    "\n",
    "# !pip install langchain #← langchain 모듈 설치\n",
    "# !pip install langchain-core #← langchain-core 모듈 설치\n",
    "# !pip install langchain-openai #← Integration 패키지에서 langchain-openai 모듈 설치\n",
    "# !pip install bs4 #← Ibs4 라이브러리 설치\n",
    "\n",
    "from langchain.chains import LLMChain, LLMRequestsChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "prompt = PromptTemplate( #← PromptTemplate을 초기화\n",
    "    template=\"\"\"아래 문장을 바탕으로 질문에 답해 주세요.\n",
    "                질문: {query}\n",
    "                문장: {requests_result}\"\"\",\n",
    "    input_variables=[\"query\",\n",
    "                \"requests_result\"],\n",
    ")\n",
    "\n",
    "# memory = ConversationBufferMemory()\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt,\n",
    "    # memory=memory, #← 메모리 기능을 사용하고 싶어도, 메모리는 input을 하나만 받음. 여기 LLMChain은 'query', 'request_result' 두 개의 input을 받기 때문에 충돌이 일어남.\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain = LLMRequestsChain(  #← LLMRequestsChain을 초기화\n",
    "    llm_chain=llm_chain,  #← llm_chain에 LLMChain을 지정\n",
    ")\n",
    "\n",
    "res = chain.invoke({\n",
    "    \"query\": \"도쿄의 날씨를 알려주세요\",\n",
    "    \"url\": \"https://www.jma.go.jp/bosai/forecast/data/overview_forecast/130000.json\",\n",
    "})\n",
    "\n",
    "print(type(res))\n",
    "print(res)\n",
    "print(res['output'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffjxBueuxUI4"
   },
   "source": [
    "### 연습문제1) LangChain을 사용하여 키워드로 뉴스를 검색하고, 가장 첫번째 뉴스의 제목과 내용을 요약하는 프로그램을 작성하세요.\n",
    "- https://newsapi.org/ 의 All articles API를 사용하여 키워드를 입력하고, 관련 뉴스를 가져오세요.\n",
    "- 가장 첫번째 뉴스의 제목과 내용을 요약하고 출력하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvWzH9NzInZt"
   },
   "outputs": [],
   "source": [
    "%%writefile api_keys.py\n",
    "# api_keys.py 파일을 새로 만들고, NEWS_API_KEY 변수에 자신의 News api 키 등록하기\n",
    "\n",
    "OPENAI_API_KEY = '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'\n",
    "NEWS_API_KEY= '572347c7fe6c4e8186ad57eae56f15f9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pl5URcFGTQNI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from api_keys import NEWS_API_KEY, OPENAI_API_KEY\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "os.environ['NEWS_API_KEY'] = NEWS_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "collapsed": true,
    "id": "VqpiXm2dxUI5",
    "outputId": "c22e7138-047b-4819-c772-5d726a10e61b"
   },
   "outputs": [],
   "source": [
    "# (연습문제1) Chains - Chains - LLMRequestsChain\n",
    "# news_api와 LLMRequestsChain을 이용하여 JSON 형태로 데이터를 가져와 내용 요약하는 기능 구현\n",
    "\n",
    "from langchain.chains import LLMChain, LLMRequestsChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import os\n",
    "\n",
    "# OpenAI LLM 초기화\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "# 프롬프트 템플릿 설정\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"requests_result\"],\n",
    "    template=\"\"\"{requests_result} 중 가장 첫 번째 뉴스의 제목과 내용을 한글로 보여줘.\n",
    "\n",
    "    제목:\n",
    "    내용요약: \"\"\"\n",
    ")\n",
    "\n",
    "# LLMChain 생성\n",
    "llm_chain = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# LLMRequestsChain 생성\n",
    "chain = LLMRequestsChain(\n",
    "    llm_chain=llm_chain\n",
    ")\n",
    "\n",
    "# API 요청 실행 (NewsAPI 사용 - API_KEY 필요)\n",
    "# NEWS_API_KEY = os.environ.get('NEWS_API_KEY')\n",
    "q = '상호관세'\n",
    "url = f\"https://newsapi.org/v2/everything?q={q}&apiKey={NEWS_API_KEY}\"\n",
    "\n",
    "res = chain.invoke({\n",
    "    \"url\": url\n",
    "})\n",
    "\n",
    "res['output']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "qw2Cg1h3BLP7",
    "outputId": "6c652596-9243-4dff-841e-5b7fa37e282c"
   },
   "outputs": [],
   "source": [
    "# (예제코드4) Chains – SimpleSequentialChain\n",
    "# SimpleSequencialChain을 이용하여 여러 Chain을 직렬로 연결하여 기능을 수행함.\n",
    "\n",
    "!pip install langchain #← langchain 모듈 설치\n",
    "!pip install langchain-core #← langchain-core 모듈 설치\n",
    "!pip install langchain-openai #← Integration 패키지에서 langchain-openai 모듈 설치\n",
    "\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "write_article_chain = LLMChain( #← 기사를 쓰는 LLMChain 만들기\n",
    "    llm=chat,\n",
    "    prompt=PromptTemplate(\n",
    "        template=\"{input}에 관한 기사를 써주세요.\",\n",
    "        input_variables=[\"input\"],\n",
    "    ),\n",
    ")\n",
    "\n",
    "translate_chain = LLMChain( #← 번역하는 LLMChain을 생성\n",
    "    llm=chat,\n",
    "    prompt=PromptTemplate(\n",
    "        template=\"다음 문장을 영어로 해 주세요.\\n{input}\",\n",
    "        input_variables=[\"input\",],\n",
    "    ),\n",
    ")\n",
    "\n",
    "sequential_chain = SimpleSequentialChain( #← SimpleSequentialChain을 생성\n",
    "    chains=[ #← 실행할 Chain을 지정\n",
    "        write_article_chain,\n",
    "        translate_chain,\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = sequential_chain.invoke({\"input\":\"배드민턴 스매싱을 강하게 하는 방법\",\n",
    "                                }) #← SimpleSequentialChain을 실행\n",
    "\n",
    "print(type(result))\n",
    "print(result)\n",
    "print(result['output'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "7RyzDC0OBLP7",
    "outputId": "43e36982-5795-4abf-b013-df946df878a7"
   },
   "outputs": [],
   "source": [
    "# (예제코드5) Chains – SimpleSequentialChain - multiple inputs\n",
    "# 여러 개의 입력 파라미터를 사용하려면, SequentialChain을 사용해야 함.\n",
    "# SimpleSequentialChain은 하나의 입력 파라미터만(input 변수) 사용할 수 있음.\n",
    "# LLMChain 안에서 output_key를 적절히 활용해야 함.\n",
    "# 이미 사용한 output_key는 다시 사용할 수 없음.\n",
    "# 그러니까 이미 \"input\" 변수를 써 먹으면, output_key=\"input\"으로는 쓸 수 없다는 것.\n",
    "\n",
    "!pip install langchain #← langchain 모듈 설치\n",
    "!pip install langchain-core #← langchain-core 모듈 설치\n",
    "!pip install langchain-openai #← Integration 패키지에서 langchain-openai 모듈 설치\n",
    "\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# 기사를 생성하는 LLMChain\n",
    "write_article_chain = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=PromptTemplate(\n",
    "        template=\"{input}에 관한 기사를 써주세요.\",\n",
    "        input_variables=[\"input\"],  # \"topic\" 입력값 사용\n",
    "    ),\n",
    "    output_key=\"article\",  # 출력값의 키(설정이라고 봐야 함.)\n",
    ")\n",
    "\n",
    "# 번역하는 LLMChain (목적 언어 추가)\n",
    "translate_chain = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=PromptTemplate(\n",
    "        template=\"다음 문장을 {target_language}로 번역해 주세요.\\n문장: {article}\",\n",
    "        input_variables=[\"article\", \"target_language\"],  # 번역할 문장과 목적 언어\n",
    "    ),\n",
    "    output_key=\"translated_text\",  # 출력값의 키\n",
    ")\n",
    "\n",
    "# SequentialChain으로 두 체인 연결\n",
    "sequential_chain = SequentialChain(\n",
    "    chains=[write_article_chain, translate_chain],  # 체인 리스트\n",
    "    input_variables=[\"input\", \"target_language\"],  # 초기 입력값\n",
    "    output_variables=[\"translated_text\"],  # 최종 출력값\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 입력 데이터\n",
    "input_data = {\n",
    "    \"input\": \"배드민턴 스매싱을 강하게 하는 방법\",  # 기사 주제\n",
    "    \"target_language\": \"영어\",       # 번역 목적 언어\n",
    "}\n",
    "\n",
    "# 체인 실행\n",
    "result = sequential_chain.invoke(input_data)\n",
    "\n",
    "# 결과 출력\n",
    "print(result)\n",
    "print(result['translated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2G23O83gOn35"
   },
   "source": [
    "# LangChain4 - Vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNBEIotQNd1v"
   },
   "source": [
    "## PDF 로드 및 페이지별로 나누기\n",
    "- langchain에서 다양한 PDFLoader를 지원하는데, 그 중 PyMuPDFLoader를 사용함.\n",
    "- 성능과 속도면에서 장점이 있음.\n",
    "- 페이지별로 Document 타입으로 저장하며, 리스트로 Document를 모아서 반환함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cjHr2D28Njmt"
   },
   "outputs": [],
   "source": [
    "# 랭체인의 PyMuPDFLoader를 사용하기 위해 pymupdf 라이브러리 설치\n",
    "\n",
    "!pip install -q pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "KX2V6WnENme3",
    "outputId": "b7bda91c-85e2-4d1c-fc4e-aca02e969fc4"
   },
   "outputs": [],
   "source": [
    "# (예제코드1) PyMuPDFLoader – PDF 파일 내 문서 추출\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(\"./tariff_war_us_china.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(type(documents[0]))\n",
    "print(len(documents))\n",
    "\n",
    "for doc in documents:\n",
    "    print(doc.metadata)\n",
    "    print(doc.page_content)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNPUHH9NNwlb"
   },
   "source": [
    "### 연습문제1) 아래 요구사항을 만족하는 코드를 작성하세요.\n",
    "- PyMuPDFLoader를 사용하여 PDF 파일(tariff_war_us_china.pdf)을 불러오세요.\n",
    "- RecursiveCharacterTextSplitter를 이용하여 문서를 500자 단위로 나누되, 중첩(overlap)은 0으로 설정하세요.\n",
    "- 힌트: chunk_size, chunk_overlap 파라미터 활용\n",
    "- Reference: https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html#langchain_text_splitters.character.RecursiveCharacterTextSplitter\n",
    "- 분할된 문서 리스트의 개수를 출력하세요.\n",
    "- 첫 번째 문서의 데이터 타입을 출력하세요.\n",
    "- 두 번째 문서의 내용을 출력하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D0czEEJDNtjM",
    "outputId": "7060c5a1-da43-425b-9dcb-bc00b5a64768"
   },
   "outputs": [],
   "source": [
    "# (연습문제1) RecursiveCharacterTextSplitter – PDF 파일 내용 추출\n",
    "# pdf 파일을 load한 데이터(documents)를 바로 split 할 수 있음.\n",
    "# split_documents(documents): 스플릿한 documents를 인자로 입력\n",
    "\n",
    "!pip install -q pymupdf #← PyMuPDF 라이브러리 설치\n",
    "!pip install -q langchain-text-splitters #← langchain의 text splitters 패키지 설치\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader #← PyMuPDFLoader 모듈 임포트\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter #← RecursiveCharacterTextSplitter를 가져옴\n",
    "\n",
    "loader = PyMuPDFLoader(\"./tariff_war_us_china.pdf\") #← PDF 파일 로더 초기화\n",
    "documents = loader.load()  #← PDF 파일을 로드하여 문서 리스트로 변환\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0) #← RecursiveCharacterTextSplitter 초기화\n",
    "splitted_documents = text_splitter.split_documents(documents) #← 문서를 분할\n",
    "\n",
    "print(len(splitted_documents)) #← 분할된 문서 개수 출력\n",
    "print(type(splitted_documents[0])) #← 첫 번째 분할 문서의 타입 출력\n",
    "print(splitted_documents[1].page_content) #← 두 번째 분할 문서의 내용 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uVJANEsN6JU"
   },
   "source": [
    "### 연습문제2) 아래 요구사항을 만족하는 코드를 작성하세요.\n",
    "- 텍스트 파일(tariff_war_us_china.txt)을 UTF-8 인코딩으로 읽어오세요.\n",
    "읽어온 텍스트의 전체 길이를 출력하세요.\n",
    "- RecursiveCharacterTextSplitter를 사용하여 문서를 500자 단위로 나누되, 중첩(overlap)은 0으로 설정하세요.\n",
    "- 분할된 텍스트 리스트의 개수를 출력하세요.\n",
    "- 두 번째 분할 텍스트의 내용을 출력하세요.\n",
    "- 힌트)\n",
    "    - with open as 구문을 활용하면 쉽게 txt 파일의 내용을 텍스트로 읽어 올 수 있습니다.\n",
    "    - RecursiveCharacterTextSplitter의 create_documents() 메서드를 이용하면 텍스트를 쉽게 문서화할 수 있어요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W8f92Y9NN_--",
    "outputId": "46d0191f-5015-452d-c94d-e083171c5303"
   },
   "outputs": [],
   "source": [
    "# (연습문제2) RecursiveCharacterTextSplitter – TXT 파일 내용 추출\n",
    "# 입력되는 파일이 txt 파일인 경우 내용을 읽어 파일 객체를 입력값으로 줌.\n",
    "# create_documents([file]): file 객체를 리스트로 입력\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "with open(\"./tariff_war_us_china.txt\", encoding=\"utf-8\") as f: #← 텍스트 파일 열기\n",
    "    file = f.read() #← 파일 내용 읽기\n",
    "print('텍스트의 길이:', len(file)) #← 텍스트 길이 출력\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0) #← RecursiveCharacterTextSplitter 초기화\n",
    "texts = text_splitter.create_documents([file]) #← 텍스트를 문서로 변환 및 분할\n",
    "\n",
    "print(len(texts)) #← 분할된 텍스트 개수 출력\n",
    "print(texts[1].page_content) #← 두 번째 분할 텍스트 내용 출력\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdRlA3tLODPl"
   },
   "source": [
    "## 벡터 임베딩과 코사인 유사도 계산\n",
    "- PDF 파일로 부터 Document를 만들고, 각 Document를 다시 Chunk 단위로 만든 다음, 그 문장 text를 token 단위로 나눈 후, 임베딩 AI를 이용하여 벡터 임베딩을 만듦.\n",
    "- 벡터 임베딩을 만들면, 문장 간 유사도를 쉽게 계산할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "leQTnaqDOEMW"
   },
   "outputs": [],
   "source": [
    "# 분할한 문서를 OpenAI embdding으로 벡터화한 후 데이터베이스에 저장\n",
    "# tiktoken 라이브러리 설치\n",
    "# tiktoken 라이브러리는 OpenAI의 GPT 모델과 같은 언어 모델에서 텍스트를 토큰 단위로 처리하기 위한 도구\n",
    "# 토큰화, 토큰 수 계산, 디코딩과 같은 작업을 효율적으로 수행하도록 설계됨.\n",
    "\n",
    "!pip install -q tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85-n_yZYOHSo",
    "outputId": "70a41e4d-d62b-4ad7-b729-41c0d9fb96c9"
   },
   "outputs": [],
   "source": [
    "# (예제코드3) 벡터 유사도 계산하기\n",
    "# 코사인 유사도 계산\n",
    "# vec1, vec2, vec3 각 pair간 유사도를 계산\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot  #← 벡터 내적 계산을 위한 함수\n",
    "from numpy.linalg import norm  #← 벡터 norm 계산을 위한 함수\n",
    "\n",
    "def cos_sim(A, B):\n",
    "    return dot(A, B) / (norm(A) * norm(B))  #← 두 벡터의 내적계산\n",
    "\n",
    "# 비교할 벡터 정의\n",
    "vec1 = np.array([0,1,1,1])  #← 첫 번째 벡터\n",
    "vec2 = np.array([1,0,1,1])  #← 두 번째 벡터\n",
    "vec3 = np.array([2,0,2,2])  #← 세 번째 벡터\n",
    "\n",
    "# 벡터 간 코사인 유사도 출력\n",
    "print('벡터1과 벡터2의 유사도 :', cos_sim(vec1, vec2))  #← vec1과 vec2의 코사인 유사도 계산\n",
    "print('벡터1과 벡터3의 유사도 :', cos_sim(vec1, vec3))\n",
    "print('벡터2와 벡터3의 유사도 :', cos_sim(vec2, vec3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3HN3XF3mOg5s",
    "outputId": "0bd515a1-7316-4638-89c7-ce2a036f5523"
   },
   "outputs": [],
   "source": [
    "# (예제코드4) OpenAIEmbeddings – 임베딩 모델 사용하기\n",
    "# 단어, 문장, 문맥을 이해하기 위해 수치 벡터로 구성된 임베딩 모델을 이용함.\n",
    "# 다양한 언어모델이 언어를 처리하는 기반에는 임베딩 모델이 존재함.\n",
    "# 여기에서는 OpenAIEmbeddings를 이용하지만, 다양한 언어 임베딩 모델이 존재함.\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm  #← 벡터의 크기(노름, norm) 계산을 위한 함수\n",
    "from langchain_openai import OpenAIEmbeddings  #← OpenAI의 임베딩 모델을 사용하기 위한 모듈\n",
    "\n",
    "# OpenAI 임베딩 모델 설정 (text-embedding-ada-002 사용)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# 텍스트 임베딩 변환 (각 문장을 벡터로 변환)\n",
    "q_emb = embeddings.embed_query(\"나는 인공지능 공부하는 게 재미있어.\")  #← 기준이 되는 문장의 임베딩 벡터\n",
    "a_emb = embeddings.embed_query(\"인공지능을 공부하긴 해야하는데...\")  #← 비교할 첫 번째 문장\n",
    "b_emb = embeddings.embed_query(\"아우 귀찮다 귀찮아.\")  #← 비교할 두 번째 문장\n",
    "\n",
    "# q_emb의 타입과 길이\n",
    "print(type(q_emb))\n",
    "print(np.array(q_emb).shape)\n",
    "\n",
    "# 코사인 유사도 계산 및 출력\n",
    "print(\"a_query_sim: \", np.dot(q_emb, a_emb) / (norm(q_emb) * norm(a_emb)))  #← q_emb와 a_emb의 코사인 유사도\n",
    "print(\"b_query_sim: \", np.dot(q_emb, b_emb) / (norm(q_emb) * norm(b_emb)))  #← q_emb와 b_emb의 코사인 유사도\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3iYjVRhnK-nl"
   },
   "source": [
    "## 벡터 DB 저장소 만들고 문서 저장하기\n",
    "- 문서나 문장을 벡터로 만든 후, 벡터 데이터베이스에 저장하여 저장 및 추출을 용이하게 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8eFqe7nPQtO",
    "outputId": "5c0fa3f8-7ce7-4c86-9c75-134d91f6176b"
   },
   "outputs": [],
   "source": [
    "# chroma db를 사용하기 위한 파이썬 라이브러리와 langchain 라이브러리 설치\n",
    "\n",
    "!pip install -q langchain-chroma chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xkrvQ-2ULVQu",
    "outputId": "c7382d6a-1815-489d-c6f0-58902753303e"
   },
   "outputs": [],
   "source": [
    "# pymupdf 라이브러리 설치\n",
    "\n",
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sp0oKTKJKoLP",
    "outputId": "a235a53a-9a4a-477b-8ea9-69bb594049c3"
   },
   "outputs": [],
   "source": [
    "# (예제코드5) 로컬 환경에 벡터 데이터베이스 만들기 - Chroma\n",
    "# Chroma db 사용\n",
    "# local 환경에 벡터 데이터베이스 생성 및 문서 추가\n",
    "\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain_openai import OpenAIEmbeddings  #← OpenAIEmbeddings 가져오기\n",
    "from langchain.text_splitter import SpacyTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma #← Chroma 가져오기\n",
    "\n",
    "loader = PyMuPDFLoader(\"./tariff_war_us_china.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "splitted_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings( #← OpenAIEmbeddings를 초기화\n",
    "    model=\"text-embedding-3-large\" #← 모델명을 지정\n",
    ")\n",
    "\n",
    "database = Chroma(  #← Chroma를 초기화\n",
    "    persist_directory=\"./.data\",  #← 영속화 데이터 저장 위치 지정\n",
    "    embedding_function=embeddings  #← 벡터화할 모델을 지정\n",
    ")\n",
    "\n",
    "database.add_documents(  #← 문서를 데이터베이스에 추가\n",
    "    splitted_documents,  #← 추가할 문서 지정\n",
    ")\n",
    "\n",
    "print(\"데이터베이스 생성이 완료되었습니다.\") #← 완료 알림\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ZBo_14oyKq7n",
    "outputId": "d5cd6478-d87b-46d4-e656-ada882c746ad"
   },
   "outputs": [],
   "source": [
    "# (예제코드6) 벡터 데이터베이스에서 문서 검색 - Chroma\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "embeddings = OpenAIEmbeddings( #← OpenAIEmbeddings를 초기화\n",
    "    model=\"text-embedding-3-large\" #← 모델명을 지정\n",
    ")\n",
    "\n",
    "database = Chroma(\n",
    "    persist_directory=\"./.data\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "print('문서의 수: ', database._collection.count())\n",
    "\n",
    "documents = database.similarity_search(\n",
    "    query=\"미중무역전쟁이 불러올 여파는 어떤 게 있을까?\",\n",
    "    k=3, #← default k = 4\n",
    "    ) #← 데이터베이스에서 유사도가 높은 문서를 통째로 가져옴\n",
    "print(f\"찾은 문서 개수: {len(documents)}\") #← 문서 개수 표시\n",
    "\n",
    "for document in documents:\n",
    "    print(f\"문서 내용: {document.page_content}\") #← 문서 내용을 표시\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTlgNTOrKz3j"
   },
   "source": [
    "### 연습문제3) 다음 프로세스에 맞춰 주어진 PDF 문서를 FAISS 벡터 데이터베이스에 저장하고, 사용자의 입력과 가장 유사한 문서를 검색하는 시스템을 구현하세요.\n",
    "- Faiss-cpu 라이브러리 설치하기\n",
    "- PDF 문서 로드하기\n",
    "- 텍스트 분할하기\n",
    "- 임베딩 벡터 생성\n",
    "- FAISS 데이터베이스에 벡터 저장\n",
    "- 사용자의 질의(query)와 가장 유사한 문서 2개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0LaDYa7nK1Cm",
    "outputId": "4d20306e-6a92-42fa-eb97-e5c2f60dcaf2"
   },
   "outputs": [],
   "source": [
    "# faiss를 사용하기 위한 faiss-cpu 라이브러리 설치\n",
    "# GPU가 지원되는 시스템에서는 faiss-gpu 설치하여 사용할 것.\n",
    "\n",
    "!pip install -q faiss-cpu #← faiss-cpu 설치. GPU가 지원되는 시스템은 faiss-gpu 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "OGUSH6r-K30j",
    "outputId": "c1699ddf-098e-47c0-f2ff-747d56d8d0f1"
   },
   "outputs": [],
   "source": [
    "# (연습문제3) 벡터 데이터베이스 문서 검색 - FAISS\n",
    "# faiss db 사용\n",
    "# local 환경에 벡터 데이터베이스 생성 및 문서 추가\n",
    "\n",
    "from langchain.document_loaders import PyMuPDFLoader  #← PDF 문서 로드를 위한 모듈\n",
    "from langchain_openai import OpenAIEmbeddings  #← OpenAI 임베딩 모델 사용을 위한 모듈\n",
    "from langchain_community.vectorstores import FAISS  #← FAISS 벡터 데이터베이스 사용을 위한 모듈\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  #← 텍스트 분할을 위한 모듈\n",
    "\n",
    "# PDF 문서 로드\n",
    "loader = PyMuPDFLoader(\"./tariff_war_us_china.pdf\")  #← PDF 파일 로드\n",
    "documents = loader.load()  #← PDF 문서를 LangChain 문서 객체로 변환\n",
    "\n",
    "# 텍스트 분할 설정 (500자 단위, 50자 중첩)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "splitted_documents = text_splitter.split_documents(documents)  #← 문서를 작은 조각으로 분할\n",
    "\n",
    "# OpenAI 임베딩 모델 설정\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")  #← OpenAI의 최신 임베딩 모델 사용\n",
    "\n",
    "# FAISS 벡터 데이터베이스 생성\n",
    "faiss_db = FAISS.from_documents(splitted_documents, embeddings)  #← 문서 임베딩 후 FAISS DB에 저장\n",
    "\n",
    "# 저장된 문서 수 확인\n",
    "print('db에 저장된 문서의 수: ', faiss_db.index.ntotal)  #← 데이터베이스에 저장된 문서 개수 출력\n",
    "\n",
    "# FAISS 벡터 데이터베이스 저장\n",
    "faiss_db.save_local(\"./faiss_index\")  #← FAISS DB를 로컬에 저장\n",
    "\n",
    "# 유사 문서 검색 함수 정의\n",
    "def search_similar_documents(query, k=3):\n",
    "    \"\"\"사용자의 쿼리와 유사한 문서를 검색하는 함수\"\"\"\n",
    "\n",
    "    # 저장된 FAISS 데이터베이스 로드\n",
    "    loaded_faiss_db = FAISS.load_local(\n",
    "        \"./faiss_index\",\n",
    "        embeddings,\n",
    "        allow_dangerous_deserialization=True  #← 안전하지 않은 역직렬화를 허용 (주의 필요)\n",
    "    )\n",
    "\n",
    "    # 입력 쿼리와 유사한 문서 검색\n",
    "    results = loaded_faiss_db.similarity_search(query, k=k)  #← 유사 문서 상위 k개 검색\n",
    "\n",
    "    # 검색된 문서 개수 출력\n",
    "    print('db에서 검색한 문서의 수: ', len(results))\n",
    "\n",
    "    # 검색된 문서 출력\n",
    "    for i, doc in enumerate(results):\n",
    "        print(f\"\\n🔍 [Top {i+1} 유사 문서]\\n\")\n",
    "        print(doc.page_content)  #← 검색된 문서 내용 출력\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# 사용자 입력 쿼리\n",
    "user_query = \"미중 무역 전쟁의 영향은?\"\n",
    "\n",
    "# 유사 문서 검색 실행\n",
    "search_similar_documents(user_query, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KaUebuUXMRsw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
