{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYKVkAY06gaz"
   },
   "source": [
    "## **웹 크롤링 개념 다지기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Puf35u3pAbHM"
   },
   "source": [
    "## **2. BeautifulSoup 기본 개념**\n",
    "\n",
    "*   BeautifulSoup 패키지 추가\n",
    "*   패키지가 없는 경우, 설치 명령어 : pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lWIiepQCAotK"
   },
   "outputs": [],
   "source": [
    "# BeautifulSoup 불러오기\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvUa62rGJprU"
   },
   "source": [
    "## **3. BeautifulSoup HTML 코드 작성**\n",
    "- html이라는 변수에 간단한 html 코드 저장\n",
    "- html 코드는 \"\"\" 사이에 입력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8wZM78ocyJfW"
   },
   "outputs": [],
   "source": [
    "# HTML 코드 작성\n",
    "html =  \"\"\"\n",
    "<html>\n",
    "\t<body>\n",
    "\t<h1 id = 'title'>Selena 파이썬 라이브러리 활용!</h1>\n",
    "\t<p id = 'body'>오늘의 주제는 웹 데이터 수집</p>\n",
    "\t<p class = 'scraping'>원하는 데이터를 직접 수집해보자</p>\n",
    "\t<p class = 'scraping'>삼성전자 일별 시세 불러오기</p>\n",
    "\t</body>\n",
    "<html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTIn8UH3KBtn"
   },
   "source": [
    "## **4. BeautifulSoup HTML 파싱**\n",
    "- html을 파이썬에서 읽을 수 있게 파싱(파이썬 객체로 변환)\n",
    "- html이라는 변수에 저장한 html 소스코드를 .parser를 붙여 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "c6VNNWXDyJTN"
   },
   "outputs": [],
   "source": [
    "# BeautifulSoup 함수를 이용하여 soup 객체 생성\n",
    "# html이라는 변수에 저장한 html 소스코드를 .parser를 붙여 변환\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzyQL9eO6vq1"
   },
   "source": [
    "## **5. BeautifulSoup 데이터를 텍스트로 반환**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSMazsfeKjbj"
   },
   "source": [
    "### **5-1. soup**\n",
    "- soup : soup의 데이터를 모두 가져와서 텍스트로 반환\n",
    "- soup.contents : soup의 데이터를 모두 가져와서 리스트로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iLodTGfFyJP5",
    "outputId": "557ac9de-2201-4ae6-b795-0dcf2472c580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<html>\n",
      "<body>\n",
      "<h1 id=\"title\">Selena 파이썬 라이브러리 활용!</h1>\n",
      "<p id=\"body\">오늘의 주제는 웹 데이터 수집</p>\n",
      "<p class=\"scraping\">원하는 데이터를 직접 수집해보자</p>\n",
      "<p class=\"scraping\">삼성전자 일별 시세 불러오기</p>\n",
      "</body>\n",
      "<html>\n",
      "</html></html>\n"
     ]
    }
   ],
   "source": [
    "# soup의 데이터를 모두 가져와서 텍스트로 반환\n",
    "for text in soup:\n",
    "  print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5h2LWGIAKxZh"
   },
   "source": [
    "### **5-2. soup.stripped_strings**\n",
    "- soup.stripped_strings : 공백도 함께 제거하여 텍스트로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z4HqXf3iydUR",
    "outputId": "25f9ea8b-b73e-48f1-f7e2-e14301552f34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selena 파이썬 라이브러리 활용!\n",
      "오늘의 주제는 웹 데이터 수집\n",
      "원하는 데이터를 직접 수집해보자\n",
      "삼성전자 일별 시세 불러오기\n"
     ]
    }
   ],
   "source": [
    "# soup의 데이터를 모두 가져와서\n",
    "# 공백도 함께 제거하여 텍스트로 반환\n",
    "for stripped_text in soup.stripped_strings:\n",
    "  print(stripped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nejJdunq643E"
   },
   "source": [
    "## **6. BeautifulSoup Find 함수**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74HnK4JDLTxf"
   },
   "source": [
    " ### **6-1. find()**\n",
    "- find 함수는 id, class, element 등을 검색 가능\n",
    "- find : 조건에 해당하는 첫 번째 정보만 검색\n",
    "> - 클래스 이름을 알 경우, class_ 형태로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mRpNnk8a5xDf",
    "outputId": "77fe3a44-4535-4e62-8cd7-977f77b7cd19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 id=\"title\">Selena 파이썬 라이브러리 활용!</h1>\n"
     ]
    }
   ],
   "source": [
    "# id 값이 'title'인 조건에 해당하는 첫 번째 정보만 검색\n",
    "title = soup.find(id='title')\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3tapo3CS56G-",
    "outputId": "f2be66dd-d396-44ea-e0ca-b7f43a536430"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"scraping\">원하는 데이터를 직접 수집해보자</p>\n"
     ]
    }
   ],
   "source": [
    "# class 값이 'scraping'인 조건에 해당하는 첫 번째 정보만 검색\n",
    "# 클래스 이름을 알 경우, class_ 형태로 사용\n",
    "scraping = soup.find(class_='scraping')\n",
    "print(scraping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPxYuS2qLkwW"
   },
   "source": [
    "### **6-2. find_all()**\n",
    " - find_all : 조건에 해당하는 모든 정보 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tPO_Iudp6hsy",
    "outputId": "30b7c235-370a-4767-a951-6b8b19732d4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p class=\"scraping\">원하는 데이터를 직접 수집해보자</p>, <p class=\"scraping\">삼성전자 일별 시세 불러오기</p>]\n"
     ]
    }
   ],
   "source": [
    "# class 값이 'scraping'인 조건에 해당하는 모든 정보 검색\n",
    "scraping_all = soup.find_all(class_='scraping')\n",
    "print(scraping_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_R9JmPZLp3S"
   },
   "source": [
    "### **6-3. string**\n",
    "- string : 태그 내부의 텍스트만 출력\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "XlGoYUqN-k9n",
    "outputId": "d4714917-0078-4f64-cb96-7ea7ae3427b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'원하는 데이터를 직접 수집해보자'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 태그 내부의 텍스트만 출력\n",
    "scraping.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uz3FNycvMdCV"
   },
   "source": [
    "## **7. BeautifulSoup 웹 크롤링 3단계 과정**\n",
    "- Request : 웹 페이지의 URL 이용해서 HTML 문서를 요청\n",
    "> - 첫 번째 단계인 Request를 위해 import urllib.request 불러오기\n",
    "- Response : 요청한 HTML 문서를 회신\n",
    "- Parsing : 태그 기반으로 파싱(일련의 문자열을 의미 있는 단위로 분해)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KehSEo_OqrpC"
   },
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# [참고 : 자율]\n",
    "다음은 BeautifulSoup 모듈을 사용하여 웹크롤링을 동작시킨 후, DataFrame 을 얻는 과정을 코드로 구현한 것입니다. 이후 내용은 자율적으로 동작시켜보고  최종적으로  웹크롤링한 결과물을 확인해보세요!\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IV8lnnn8_zJ2"
   },
   "source": [
    "## **8. BeautifulSoup F12(개발자 도구) URL 찾기**\n",
    "- 1) 네이버 금융 홈페이지 접속 : https://finance.naver.com/\n",
    "- 2) 삼성전자(code : 005930) 검색\n",
    "- 3) 시세 메뉴 클릭 후 URL 확인 : https://finance.naver.com/item/sise.naver?code=005930\n",
    "- 4) 키보드 F12(개발자 도구) 클릭 > 메뉴 Elements 클릭 > 키보드 Ctrl과 F (검색 단축기) 클릭 > '일별 시세' 검색 > scr 값 복사  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIakImtQ7QWR"
   },
   "source": [
    "## **9. BeautifulSoup 첫 번째 단계. Request**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2VU8o3qSA3uJ"
   },
   "outputs": [],
   "source": [
    "# 웹 페이지의 URL을 이용해서 HTML 문서를 요청하기 위해 필요한 라이브러리\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91Th9jk0vl2L"
   },
   "source": [
    "### **9-1. URL 저장**\n",
    "- stock_url이라는 변수에 네이버 금융 사이트의 삼성전자 시세 정보가 담긴 URL 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "I4sMCjdzNNy9"
   },
   "outputs": [],
   "source": [
    "# stock_url이라는 변수에 URL 저장\n",
    "stock_url = 'https://finance.naver.com//item/sise_day.naver?code=005930'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZa44ymTwA0_"
   },
   "source": [
    "### **9-2. User-agent 설정**\n",
    "- user-agent 확인 사이트 : http://www.useragentstring.com/\n",
    "- user-agent란, 웹 크롤링을 진행하면 종종 페이지에서 아무것도 받아오지 못하는 경우 발생! 이유는 대부분 서버에서 봇을 차단하기 때문\n",
    "- 그래서 user-agent를 headers에 저장하면 오류를 해결할 수 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "gzosBm6rNNwh"
   },
   "outputs": [],
   "source": [
    "# header에 user-agent 값 저장\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxkokBIIvL5A"
   },
   "source": [
    "### **9-3. requests.get()**\n",
    "- 웹 페이지의 URL 이용해서 HTML 문서를 요청\n",
    "- requests.get(stock_url, headers = headers)\n",
    "> - URL 값을 파라미터 값으로 입력\n",
    "> - 해당 사이트는 반드시 헤더 정보를 요구하기 때문에 파라미터 값으로 헤더 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v1o179F-YMl6",
    "outputId": "adcda1af-1d1a-477f-bab8-f6b386ff346b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# request.get 함수를 통해 URL를 이용하여 HTML 문서를 요청\n",
    "requests.get(stock_url, headers = headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0xiDK8Owoab"
   },
   "source": [
    "## **10. BeautifulSoup 두 번째 단계. Response**\n",
    "- 요청한 HTML 문서를 회신\n",
    "- response = requests.get(stock_url, headers = headers)\n",
    "> - 서버에서 요청을 받아 처리한 후, 요청자에게 응답 줌\n",
    "> - HTML 코드 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "doKWLYgvv6GG"
   },
   "outputs": [],
   "source": [
    "# response 변수에 요청한 HTML 문서를 회신하여 저장\n",
    "response = requests.get(stock_url, headers = headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Visw_c57xfaG"
   },
   "source": [
    "## **11. BeautifulSoup 세 번째 단계. Parsing**\n",
    "- 태그 기반으로 파싱(일련의 문자열을 의미 있는 단위로 분해)\n",
    "- soup = BeautifulSoup(response.text, 'html.parser')\n",
    "> - html을 파이썬에서 읽을 수 있게 파싱(파이썬 객체로 변환)\n",
    "> - Response.text에 저장한 html 소스코드를 .parser를 붙여 변환\n",
    "> - parser는 파이썬의 내장 메소드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "UTxswDpsT7NL"
   },
   "outputs": [],
   "source": [
    "# soup 변수에 BeautifulSoup의 객체 생성\n",
    "# HTML 코드를 파이썬에서 읽을 수 있도록 파싱\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bz-qnkLQBg2j"
   },
   "source": [
    "## **12. BeautifulSoup 반복문으로 일별 종가 구현**\n",
    "- 1) 200일 일별 종가 정보는 1 Page 당 10일의 일별 종가 정보 담겨있어서 20 Page 필요\n",
    "- 2) 일별 종가 담긴 URL과 Header 정보로 requests.get 함수 구현\n",
    "- 3) 요청한 HTML 문서를 회신하여 response 변수에 저장\n",
    "- 4) BeautifulSoup함수로 HTML을 읽을 수 있도록 파싱하여 soup 변수에 저장\n",
    "- 5) Page 개수만큼 20번 반복\n",
    "> - \"tr\" 태그 조건에 해당하는 모든 정보를 검색하여 parsing_list 변수에 저장\n",
    "- 6) 1 Page 당 10일의 일별 종가 정보 담겨있어서 10번 반복\n",
    "> - \"td\" 태그의 align가 \"center\"인 값들 중 0번째 조건에 해당하는 정보 검색하여 출력\n",
    "> - \"td\" 태그의 class가 \"num\"인 값들 중 0번째 조건에 해당하는 정보 검색하여 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8Oiy7xRJM9-"
   },
   "source": [
    "**태그 정보는 F12(개발자 도구) 클릭하여 찾기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y4-YJETfn0Ok",
    "outputId": "a0e79629-ddcd-47f9-f97f-4546d82d4646"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2025.04.04 56,100\n",
      "2025.04.03 57,600\n",
      "2025.04.02 58,800\n",
      "2025.04.01 58,800\n",
      "2025.03.31 57,800\n",
      "2025.03.28 60,200\n",
      "2025.03.27 61,800\n",
      "2025.03.26 61,400\n",
      "2025.03.25 59,800\n",
      "2025.03.24 60,500\n",
      "2\n",
      "2025.03.21 61,700\n",
      "2025.03.20 60,200\n",
      "2025.03.19 58,500\n",
      "2025.03.18 57,600\n",
      "2025.03.17 57,600\n",
      "2025.03.14 54,700\n",
      "2025.03.13 54,700\n",
      "2025.03.12 54,900\n",
      "2025.03.11 53,600\n",
      "2025.03.10 53,700\n",
      "3\n",
      "2025.03.07 53,700\n",
      "2025.03.06 54,300\n",
      "2025.03.05 54,000\n",
      "2025.03.04 54,500\n",
      "2025.02.28 54,500\n",
      "2025.02.27 56,300\n",
      "2025.02.26 56,600\n",
      "2025.02.25 57,200\n",
      "2025.02.24 57,300\n",
      "2025.02.21 58,200\n",
      "4\n",
      "2025.02.20 58,400\n",
      "2025.02.19 58,700\n",
      "2025.02.18 56,900\n",
      "2025.02.17 56,000\n",
      "2025.02.14 56,000\n",
      "2025.02.13 55,800\n",
      "2025.02.12 55,800\n",
      "2025.02.11 55,700\n",
      "2025.02.10 55,600\n",
      "2025.02.07 53,700\n",
      "5\n",
      "2025.02.06 54,000\n",
      "2025.02.05 52,900\n",
      "2025.02.04 52,700\n",
      "2025.02.03 51,000\n",
      "2025.01.31 52,400\n",
      "2025.01.24 53,700\n",
      "2025.01.23 53,700\n",
      "2025.01.22 54,300\n",
      "2025.01.21 53,500\n",
      "2025.01.20 53,400\n",
      "6\n",
      "2025.01.17 53,700\n",
      "2025.01.16 54,300\n",
      "2025.01.15 53,700\n",
      "2025.01.14 53,900\n",
      "2025.01.13 54,100\n",
      "2025.01.10 55,300\n",
      "2025.01.09 56,100\n",
      "2025.01.08 57,300\n",
      "2025.01.07 55,400\n",
      "2025.01.06 55,900\n",
      "7\n",
      "2025.01.03 54,400\n",
      "2025.01.02 53,400\n",
      "2024.12.30 53,200\n",
      "2024.12.27 53,700\n",
      "2024.12.26 53,600\n",
      "2024.12.24 54,400\n",
      "2024.12.23 53,500\n",
      "2024.12.20 53,000\n",
      "2024.12.19 53,100\n",
      "2024.12.18 54,900\n",
      "8\n",
      "2024.12.17 54,200\n",
      "2024.12.16 55,600\n",
      "2024.12.13 56,100\n",
      "2024.12.12 55,900\n",
      "2024.12.11 54,000\n",
      "2024.12.10 54,000\n",
      "2024.12.09 53,400\n",
      "2024.12.06 54,100\n",
      "2024.12.05 53,700\n",
      "2024.12.04 53,100\n",
      "9\n",
      "2024.12.03 53,600\n",
      "2024.12.02 53,600\n",
      "2024.11.29 54,200\n",
      "2024.11.28 55,500\n",
      "2024.11.27 56,300\n",
      "2024.11.26 58,300\n",
      "2024.11.25 57,900\n",
      "2024.11.22 56,000\n",
      "2024.11.21 56,400\n",
      "2024.11.20 55,300\n",
      "10\n",
      "2024.11.19 56,300\n",
      "2024.11.18 56,700\n",
      "2024.11.15 53,500\n",
      "2024.11.14 49,900\n",
      "2024.11.13 50,600\n",
      "2024.11.12 53,000\n",
      "2024.11.11 55,000\n",
      "2024.11.08 57,000\n",
      "2024.11.07 57,500\n",
      "2024.11.06 57,300\n",
      "11\n",
      "2024.11.05 57,600\n",
      "2024.11.04 58,700\n",
      "2024.11.01 58,300\n",
      "2024.10.31 59,200\n",
      "2024.10.30 59,100\n",
      "2024.10.29 59,600\n",
      "2024.10.28 58,100\n",
      "2024.10.25 55,900\n",
      "2024.10.24 56,600\n",
      "2024.10.23 59,100\n",
      "12\n",
      "2024.10.22 57,700\n",
      "2024.10.21 59,000\n",
      "2024.10.18 59,200\n",
      "2024.10.17 59,700\n",
      "2024.10.16 59,500\n",
      "2024.10.15 61,000\n",
      "2024.10.14 60,800\n",
      "2024.10.11 59,300\n",
      "2024.10.10 58,900\n",
      "2024.10.08 60,300\n",
      "13\n",
      "2024.10.07 61,000\n",
      "2024.10.04 60,600\n",
      "2024.10.02 61,300\n",
      "2024.09.30 61,500\n",
      "2024.09.27 64,200\n",
      "2024.09.26 64,700\n",
      "2024.09.25 62,200\n",
      "2024.09.24 63,200\n",
      "2024.09.23 62,600\n",
      "2024.09.20 63,000\n",
      "14\n",
      "2024.09.19 63,100\n",
      "2024.09.13 64,400\n",
      "2024.09.12 66,300\n",
      "2024.09.11 64,900\n",
      "2024.09.10 66,200\n",
      "2024.09.09 67,500\n",
      "2024.09.06 68,900\n",
      "2024.09.05 69,000\n",
      "2024.09.04 70,000\n",
      "2024.09.03 72,500\n",
      "15\n",
      "2024.09.02 74,400\n",
      "2024.08.30 74,300\n",
      "2024.08.29 74,000\n",
      "2024.08.28 76,400\n",
      "2024.08.27 75,800\n",
      "2024.08.26 76,100\n",
      "2024.08.23 77,700\n",
      "2024.08.22 78,300\n",
      "2024.08.21 78,300\n",
      "2024.08.20 78,900\n",
      "16\n",
      "2024.08.19 78,300\n",
      "2024.08.16 80,200\n",
      "2024.08.14 77,200\n",
      "2024.08.13 76,100\n",
      "2024.08.12 75,500\n",
      "2024.08.09 74,700\n",
      "2024.08.08 73,400\n",
      "2024.08.07 74,700\n",
      "2024.08.06 72,500\n",
      "2024.08.05 71,400\n",
      "17\n",
      "2024.08.02 79,600\n",
      "2024.08.01 83,100\n",
      "2024.07.31 83,900\n",
      "2024.07.30 81,000\n",
      "2024.07.29 81,200\n",
      "2024.07.26 80,900\n",
      "2024.07.25 80,400\n",
      "2024.07.24 82,000\n",
      "2024.07.23 83,900\n",
      "2024.07.22 83,000\n",
      "18\n",
      "2024.07.19 84,400\n",
      "2024.07.18 86,900\n",
      "2024.07.17 86,700\n",
      "2024.07.16 87,700\n",
      "2024.07.15 86,700\n",
      "2024.07.12 84,400\n",
      "2024.07.11 87,600\n",
      "2024.07.10 87,800\n",
      "2024.07.09 87,800\n",
      "2024.07.08 87,400\n",
      "19\n",
      "2024.07.05 87,100\n",
      "2024.07.04 84,600\n",
      "2024.07.03 81,800\n",
      "2024.07.02 81,800\n",
      "2024.07.01 81,800\n",
      "2024.06.28 81,500\n",
      "2024.06.27 81,600\n",
      "2024.06.26 81,300\n",
      "2024.06.25 80,800\n",
      "2024.06.24 80,600\n",
      "20\n",
      "2024.06.21 80,000\n",
      "2024.06.20 81,600\n",
      "2024.06.19 81,200\n",
      "2024.06.18 79,800\n",
      "2024.06.17 78,100\n",
      "2024.06.14 79,600\n",
      "2024.06.13 78,600\n",
      "2024.06.12 76,500\n",
      "2024.06.11 75,200\n",
      "2024.06.10 75,700\n"
     ]
    }
   ],
   "source": [
    "# 200일 동안의 일별 종가 정보 가져오는 반복문(1페이지 당 10일 정보 담겨있음)\n",
    "for page in range(1, 21):\n",
    "  print (str(page))\n",
    "\n",
    "  # url + page 번호 합치기\n",
    "  stock_url = 'http://finance.naver.com/item/sise_day.nhn?code=005930' +'&page='+ str(page)\n",
    "\n",
    "  # header 정보\n",
    "  headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "\n",
    "  # request : 웹 페이지의 URL, header 이용해서 HTML 문서 요청\n",
    "  # response : 요청한 HTML 문서 회신\n",
    "  response = requests.get(stock_url, headers = headers)\n",
    "\n",
    "  # parsing : HTML을 읽을 수 있도록 파싱\n",
    "  # soup 변수에 BeautifulSoup의 객체 생성\n",
    "  soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "  # \"tr\" 태그 조건에 해당하는 모든 정보 검색\n",
    "  parsing_list = soup.find_all(\"tr\")\n",
    "\n",
    "  # None 값은 걸러주기 위한 변수 생성\n",
    "  isCheckNone = None\n",
    "\n",
    "  # 페이지당 일별 종가 출력하기 위한 반복문 <들여쓰기 주의>\n",
    "  for i in range(1, len(parsing_list)):\n",
    "    # None 값은 걸러주기 위한 조건문 <들여쓰기 주의>\n",
    "    # .span()는 매치된 문자열의 (시작, 끝)에 해당하는 튜플을 돌려주는 함수\n",
    "    if(parsing_list[i].span != isCheckNone):\n",
    "      # parsing_list[i] : i번째 parsing_list, i 번째 \"tr\" 태그 값\n",
    "      # .find_all(\"td\", align=\"center\")[0].text : \"td\" 태그의 align가 \"center\"인 값들 중 0번째 값\n",
    "      # .find_all(\"td\", class_=\"num\")[0].text : \"td\" 태그의 class가 \"num\"인 값들 중 0번째 값\n",
    "      print(parsing_list[i].find_all(\"td\", align=\"center\")[0].text,\n",
    "            parsing_list[i].find_all(\"td\", class_=\"num\")[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "by9wQrKtMol6"
   },
   "source": [
    "## **13. Pandas 일별 시세 테이블 구현**\n",
    "- 1) Pandas 라이브러리와 Requests 라이브러리 이용\n",
    "- 2) 200일 일별 종가 정보는 1 Page 당 10일의 일별 종가 정보 담겨있어서 20 Page 필요\n",
    "- 3) 일별 종가 담긴 URL과 Header 정보로 requests.get 함수 구현\n",
    "- 4) pandas.read_html 함수를 통해 HTML 불러와서 파싱\n",
    "- 5) append 함수를 이용하여 dataframe 끝에 추가하고 싶은 요소를 추가하여 dataframe 리턴\n",
    "- 6) dropna 함수를 통해 결측 값 제거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "Brb_XqwbxHEb",
    "outputId": "c5f23268-a55f-439a-d945-4b883208de3a"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'lxml'.  Use pip or conda to install lxml.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\envs\\tensor\\Lib\\site-packages\\pandas\\compat\\_optional.py:135\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\envs\\tensor\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1310\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1324\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'lxml'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     17\u001b[39m     response = requests.get(stock_url, headers = headers)\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# response.text로 응답을 주면 HTML 코드이기 때문에 read_html로 불러오기\u001b[39;00m\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# append() : dataframe 끝에 추가하고 싶은 요소를 추가하여 새로운 dataframe을 리턴\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     df1 = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mStringIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     22\u001b[39m     df = pd.concat([df, df1], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# 결측 값 있는 행 제거\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\envs\\tensor\\Lib\\site-packages\\pandas\\io\\html.py:1240\u001b[39m, in \u001b[36mread_html\u001b[39m\u001b[34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend, storage_options)\u001b[39m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m   1225\u001b[39m     [\n\u001b[32m   1226\u001b[39m         is_file_like(io),\n\u001b[32m   (...)\u001b[39m\u001b[32m   1230\u001b[39m     ]\n\u001b[32m   1231\u001b[39m ):\n\u001b[32m   1232\u001b[39m     warnings.warn(\n\u001b[32m   1233\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing literal html to \u001b[39m\u001b[33m'\u001b[39m\u001b[33mread_html\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1234\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in a future version. To read from a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1240\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisplayed_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisplayed_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_links\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1259\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\envs\\tensor\\Lib\\site-packages\\pandas\\io\\html.py:971\u001b[39m, in \u001b[36m_parse\u001b[39m\u001b[34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[39m\n\u001b[32m    969\u001b[39m retained = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    970\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m flav \u001b[38;5;129;01min\u001b[39;00m flavor:\n\u001b[32m--> \u001b[39m\u001b[32m971\u001b[39m     parser = \u001b[43m_parser_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflav\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    972\u001b[39m     p = parser(\n\u001b[32m    973\u001b[39m         io,\n\u001b[32m    974\u001b[39m         compiled_match,\n\u001b[32m   (...)\u001b[39m\u001b[32m    979\u001b[39m         storage_options,\n\u001b[32m    980\u001b[39m     )\n\u001b[32m    982\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\envs\\tensor\\Lib\\site-packages\\pandas\\io\\html.py:918\u001b[39m, in \u001b[36m_parser_dispatch\u001b[39m\u001b[34m(flavor)\u001b[39m\n\u001b[32m    916\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mbs4\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m     \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlxml.etree\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _valid_parsers[flavor]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\envs\\tensor\\Lib\\site-packages\\pandas\\compat\\_optional.py:138\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Missing optional dependency 'lxml'.  Use pip or conda to install lxml."
     ]
    }
   ],
   "source": [
    "# 네이버 금융 일별 시세 테이블 불러오기\n",
    "# pandas 라이브러리와 requests 라이브러리 이용\n",
    "# code = 회사 코드, page = 일별 시세 테이블의 페이지 수 (200 행의 데이터 불러오려면 20 페이지 입력)\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for page in range(1,21):\n",
    "    stock_url = 'http://finance.naver.com/item/sise_day.nhn?code=005930' +'&page='+ str(page)\n",
    "\n",
    "    # header 정보\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "\n",
    "    # request : 웹 페이지의 URL, header 이용해서 HTML 문서 요청\n",
    "    # response : 요청한 HTML 문서 회신\n",
    "    response = requests.get(stock_url, headers = headers)\n",
    "\n",
    "    # response.text로 응답을 주면 HTML 코드이기 때문에 read_html로 불러오기\n",
    "    # append() : dataframe 끝에 추가하고 싶은 요소를 추가하여 새로운 dataframe을 리턴\n",
    "    df1 = pd.read_html(StringIO(response.text), header=0)[0]\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "\n",
    "# 결측 값 있는 행 제거\n",
    "df = df.dropna()\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
